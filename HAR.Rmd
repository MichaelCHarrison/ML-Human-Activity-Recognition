---
title: "Human Activity Recognition Predictive Analysis"
output: html_notebook
---

*Problem Description*

Wearable technologies (such as Fitbit, Nike FuelBand and Garvmin Vivosmart products) have allowed for the widespread collection of data relating to physical activity. While the focus of these devices is largely on quantifying and tracking the amount of activity, the data collected carries the potential to assess not only quantifiable measures but qualitative as well. Human Activity Recognition (HAR) has hained increasing attention in the research community for the development of potential applications in context-aware systems. 

Using the dataset from Ugulino et al.'s "Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements", in which subjects performed barbell lifts correctly and incorrectly to establish 5 different classes of activity, I will use machine learning based classifer to predict the class of acitivy for the data set.

*Provided Data*
```{r, warning=FALSE, message=FALSE}
library(caret)
library(data.table)

training <- fread("pml-training.csv")
testing <- fread("pml-testing.csv")
dim(training)
```



- Data Constraints
The 19,622 observations of 160 variables contains many variables unnecessary in model building, namely the first 7 columns pertaining to observation number, subject name, timestamps, and window variable. Additionally, derived column variables containing missing data or NA values (kurtosis, skewness, max, min, variance, standard deviation, amplitude, and average of sensors) will be removed, preserving only measurements from belt, arm, formarm, and dumbbell.

```{r, cache=TRUE}
#remove NA columns; max, min, variance, sd, amplitude, average
trainingPrime <- t(na.omit(t(training)))
#remove observation number, subject name, timestamps
trainingPrime <- trainingPrime[, -c(1:7)]
#remove kurtosis, skewness, yaw column variables
toDrop <- c("^kurtosis*","^skewness*","yaw")
trainingPrime <- trainingPrime[, -grep(paste(toDrop, collapse = "|"),
                                       colnames(trainingPrime))]
#transform matrix to data.table
trainingPrime <- as.data.table(trainingPrime)
```

Column types for numerical measure need to be converted from character classes to numeric for modeling purposes:

```{r}
trainingPrime[,1:48] <- trainingPrime[, lapply(trainingPrime[,1:48], as.numeric)]
trainingPrime$classe <- as.factor(trainingPrime$classe)
dim(trainingPrime)
```

*Analyze Data*
- Summarize
        - Data Structure
        - Data Distributions
- Visualize
        - Atrribute Histogram
        
```{r}

```

        - Pairwise scatter plot
```{r}

```

*Prepare Data*
- Formatting
- Cleaning
- Sampling

~Sampling
```{r}
inTrain <- createDataPartition(y=trainingPrime$classe,
                               p=.75, list=FALSE)
training1 <- trainingPrime[inTrain,]
testing1 <- trainingPrime[-inTrain,]
```

*Evaluate Algorithm*

- Select Algorithm
        - Interpret and report results


#RF, mtry = 7#
```{r}
#Configur parallel processing
start.time <- Sys.time()
library(parallel)
library(doParallel)
library(randomForest)

cluster <- makeCluster(detectCores()-1)
registerDoParallel(cluster)

####################Run default first#############################
fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 3,
                           allowParallel = TRUE)

set.seed(1704111)
fitRF1 <- train(classe~., data=training1, 
                method="rf", 
                metric = "Accuracy", 
                trControl = fitControl, 
                tuneGrid = expand.grid(mtry=7))

stopCluster(cluster)
registerDoSEQ()
end.time <- Sys.time()
time.takenRF1 <- end.time - start.time #Time difference of 8.76054 mins
fitRF1
```


#Resample Stats#
```{r}
fitRF1$resample
```


#Confustion Matrix#
```{r}
confusionMatrix(fitRF1)
```


*Improve Results*
- Algorithm Tuning
        - Using the tunRF function of randomForest to optimize mtry value
        
        
        
#Tuning#
```{r}
x <- training1[,1:48]
y <- training1$classe
set.seed(1704111)
time.start <- Sys.time()
optmtry <- tuneRF(x, y, 
                  stepFactor = 1.5, 
                  improve = 1e-5, ntreeTry = 500)
time.end <- Sys.time()
time.takenRF2 <- time.end - time.start
optmtry
```

#Applied Tuning#
```{r}
cluster <- makeCluster(detectCores()-1)
registerDoParallel(cluster)

#Recommended default control features
fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 3,
                           allowParallel = TRUE)

set.seed(1704111)
fitRF3 <- train(classe~., data=training1, 
                method="rf", 
                metric = "Accuracy", 
                trControl = fitControl, 
                tuneGrid = expand.grid(mtry=6))

stopCluster(cluster)
registerDoSEQ()
end.time <- Sys.time()
time.takenRF3 <- end.time - start.time
fitRF3
```








##Scrapped Scripts##
```{r}
#Configur parallel processing
start.time <- Sys.time()
library(parallel)
library(doParallel)
library(randomForest)

cluster <- makeCluster(detectCores()-1)
registerDoParallel(cluster)

#search = "random"
fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 3,
                           search = "random",
                           allowParallel = TRUE)

set.seed(1704111)
fitRF2 <- train(classe~., data=training1, 
                method="rf", 
                metric = "Accuracy", 
                trControl = fitControl, 
                tuneLength = 15)

stopCluster(cluster)
registerDoSEQ()
end.time <- Sys.time()
time.takenRF2 <- end.time - start.time
fitRF2
```

```{r}
start.time <- Sys.time()
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores()-1)
registerDoParallel(cluster)

inTrain2 <- createDataPartition(y=trainingPrime$classe,
                               p=.75, list=FALSE)
training2 <- trainingPrime[inTrain2,]
testing2 <- trainingPrime[-inTrain2,]

gbmGrid <- expand.grid(shrinkage = .1,
                       n.trees = (1:30)*50,
                       interaction.depth = c(1,5,9),
                       n.minobsinnode = 20)

fitControl2 <- trainControl(method = "cv",
                            number = 10,
                            allowParallel = TRUE)
set.seed(1704112)
fit2 <- train(classe~., data=training2,
              method="gbm", trControl = fitControl2,
              verbose=FALSE, tuneGrid=gbmGrid)
stopCluster(cluster)
registerDoSEQ()
end.time <- Sys.time()
time.taken <- end.time - start.time
fit2

```


*Applying randomForest model to testing set*
```{r}
pred1 <- predict(fit1, testing1)
testing1$predRight <- pred1 == testing1$classe
table(pred1, testing1$classe)
```
```{r}
sum(testing1$predRight)/length(testing1$classe)
```

