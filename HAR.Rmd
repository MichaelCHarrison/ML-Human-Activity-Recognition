---
title: "Human Activity Recognition Predictive Analysis"
author: Michael Harrison
output: 
        md_document: 
                variant: markdown_github
---

## Problem Description

This study will use the data from Ugulino et al.'s "Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements", in which subjects performed barbell lifts correctly and incorrectly to establish 5 different classes of activity. RandomForest will be the machine learning classifier used to predict the class of activity for the data set.

### Data
```{r, cache = TRUE, warning=FALSE, message=FALSE}
library(caret)
library(data.table)

trainingPrime <- fread("pml-training.csv")
dim(trainingPrime)
```

#### Data Constraints
The 19,622 observations of 160 variables contains many variables unnecessary in model building, namely: 
- first 7 columns pertaining to observation number
- subject name
- timestamps 
- window variable  
  
Additionally, derived column variables containing missing data or NA values will be removed:  

- kurtosis
- skewness
- max
- min
- variance
- standard deviation
- amplitude
- average of sensors  

Only measurements from the belt, arm, formarm, and dumbbell varables will be preserved and will be converted from character classes to numeric for modeling purposes.  


# Set this as a function to clean the test function
```{r, cache=TRUE}

dataPrep <- function(data){
        #remove NA columns; max, min, variance, sd, amplitude, average
        prep <- t(na.omit(t(data)))

        #remove observation number, subject name, timestamps
        prep <- prep[, -c(1:7)]
        
        #remove kurtosis, skewness, yaw column variables
        toDrop <- c("^kurtosis*","^skewness*","yaw")
        prep <- prep[, -grep(paste(toDrop, collapse = "|"), colnames(prep))]
        
        #transform matrix to data.table
        prep <- as.data.table(prep)
        #Convert character class values to numeric; 49th variable (classe) set to factor
        prep[,1:48] <- prep[, lapply(prep[,1:48], as.numeric)]
        prep$classe <- as.factor(prep$classe)
        prep
}

preppedData <- dataPrep(trainingPrime)
dim(preppedData)
```

The training dataset now conatains 48 numeric predictors and 1 factor outcome. 

```{r}
library(caret)

inTrain <- createDataPartition(y=preppedData$classe,
                               p=.5, list=FALSE)
training <- preppedData[inTrain,]
validation_test <- preppedData[-inTrain,]

inVal <- createDataPartition(y=validation_test$classe,
                             p = .5, list=FALSE)
validation <- validation_test[inVal]
testing <- validation_test[-inVal]
```
```{r}
dim(training)
```
```{r}
dim(validation)
```
```{r}
dim(testing)
```


## Analyze Data

```{r, warning=FALSE, cache = TRUE, results= 'hold'}
library(ggplot2)
ggplot(preppedData, aes(classe)) + geom_histogram(stat="count")
```

```{r, cache = TRUE}
summary <- summary(training$classe)
summary
```

The outcome classes are imbalanced and may need to balanced to improve prediction. The issue of outcome class imbalance will be revisited if the accuracy of the model does not meet the goal. Should the imbalance need to be corrected, the A classifer will be reduced to the mean of classes B through E.  
        
        
## Evaluating RandomForest

To predict HAR, the caret and randomForest (RF) packages will be employed. The trainingPrime dataset will be partitioned to create training and validation sets to create the RF model and validate it before being applied to the test set.


```{r}
# Set predictors and outcome to x and y respectively for brevity in coding
x <- training[,1:48]
y <- training$classe
seed <- 170418
```

```{r}
#Configures parallel processing to decreased computation time
library(parallel)
library(doParallel)

paraOn <- function(){
        cluster <- makeCluster(detectCores()-1)
        registerDoParallel(cluster)}

paraOff <- function(){
        stopCluster(cluster)
        registerDoSEQ()}
```

### Establishing baseline RF model
```{r, cache=TRUE, results='hold'}
fitControl <- trainControl(allowParallel = TRUE)

paraOn()
start.time <- Sys.time()
set.seed(seed)
fitRF.base <- train(x, y, data = training,
                      method = "rf", metric = "Accuracy",
                      trControl = fitControl)
paraOff()
elapsedRF.base <- end.time - Sys.time() 
fitRF.base
```



```{r, cache = TRUE}
fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 3,
                           allowParallel = TRUE)

start.time <- Sys.time()
paraOn()
set.seed(seed)
fitRF <- train(x, y, data=training1, 
               method="rf", 
               metric = "Accuracy",   
               trControl = fitControl,
               tuneGrid = expand.grid(mtry=round(sqrt(ncol(x)))))

paraOff()
end.time <- Sys.time()
time.takenRF <- end.time - start.time
fitRF$results
```

*Improve Results*
To improve the accuracy of the randomForest, I'll use the tuneRF function to optimize the mtry value.


```{r}
start.time <- Sys.time()
paraOn()

set.seed(seed)
optmtryRF2 <- tuneRF(x, y, 
                  stepFactor = 1.5, 
                  improve = 1e-4, ntreeTry = 500,
                  plot = TRUE)

paraOff()
end.time <- Sys.time()
time.takenRF2 <- end.time - start.time
optmtryRF2
```

The Out of Bag error as dependent on mtry seems to hold relatively constant between 6 and 9. As the rounded value of the square root of the number of columns comes out to 7, it doesn't appear that a changing the mtry value will return considerable change to the accuracy of the model.


Holding mtry constant, I will create a set of models with different ntree values to ascertain its ideal value.

```{r, cache = TRUE}
start.time <- Sys.time()
paraOn()

fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 3,
                           allowParallel = TRUE)
#keep mtry constant 
tuneGrid <- expand.grid(mtry = round(sqrt(ncol(x))))

#tune the algorithm for ntrees
fitList <- list()
for(ntree in c(300, 400, 500, 600, 700)){
        set.seed(seed)
        fitRF <- train(x, y, data = training1,
                       method = "rf", metric = "Accuracy", 
                       tuneGrid = tuneGrid, trControl = fitControl,
                       ntree = ntree)
        key <- toString(ntree)
        fitList[[key]] <- fitRF
}

paraOff()
end.time <- Sys.time()
time.takenRFtune <- end.time - start.time
results <- resamples(fitList)
summary(results)
```


```{r}
start.time <- Sys.time()
paraOn()

fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 3,
                           allowParallel = TRUE)
#keep mtry constant 
tuneGrid <- expand.grid(mtry = round(sqrt(ncol(x))))

#tune the algorithm for ntrees
fitList2 <- list()
for(ntree in c(1000, 1500, 2000)){
        set.seed(seed)
        fitRF <- train(x, y, data = training1,
                       method = "rf", metric = "Accuracy", 
                       tuneGrid = tuneGrid, trControl = fitControl,
                       ntree = ntree)
        key <- toString(ntree)
        fitList[[key]] <- fitRF
}

paraOff()
end.time <- Sys.time()
time.takenRFtune <- end.time - start.time

results2 <- resamples(fitList)
summary(results2)
```



```{r}
start.time <- Sys.time()
paraOn()

fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 3,
                           allowParallel = TRUE)
#keep mtry constant 
tuneGrid <- expand.grid(mtry = c(6:9))

#tune the algorithm for ntrees
fitList3 <- list()
for(ntree in c(700, 2000)){
        set.seed(seed)
        fitRF3 <- train(x, y, data = training1,
                       method = "rf", metric = "Accuracy", 
                       tuneGrid = tuneGrid, trControl = fitControl,
                       ntree = ntree)
        key <- toString(ntree)
        fitList3[[key]] <- fitRF3
}

paraOff()
end.time <- Sys.time()
time.takenRFtune <- end.time - start.time

results3 <- resamples(fitList3)
fitList3
```

